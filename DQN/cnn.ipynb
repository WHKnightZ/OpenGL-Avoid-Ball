{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "data = np.array(df)\n",
    "\n",
    "x = data[:, :6700]\n",
    "y = data[:, 6700]\n",
    "\n",
    "x = x.reshape(2969, 6700)\n",
    "y = np.array([lst[int(i)] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(6700, )))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "epochs = 25\n",
    "lr = 0.001\n",
    "decay = lr / epochs\n",
    "sgd = SGD(lr=lr, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.argmax(model.predict_on_batch(np.random.rand(1, 6700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x[100].reshape(67, 100, 1)\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "\n",
    "model = load_model(\"model.h5\")\n",
    "model.predict_on_batch(np.random.rand(1, 6700))\n",
    "\n",
    "UDP_IP = \"127.0.0.1\"\n",
    "UDP_PORT_RECV = 9003\n",
    "UDP_PORT_SEND = 9004\n",
    "\n",
    "\n",
    "send = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "recv = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "recv.bind((UDP_IP, UDP_PORT_RECV))\n",
    "\n",
    "send.sendto(struct.pack(\"i\", 0), (UDP_IP, UDP_PORT_SEND))\n",
    "\n",
    "while True:\n",
    "    data, addr = recv.recvfrom(26800)\n",
    "    inp = struct.unpack('6700f', data)\n",
    "    inp = np.array(inp)\n",
    "    inp = inp.reshape(1, 6700)\n",
    "    ret = model.predict_on_batch(inp)\n",
    "    ret = np.argmax(ret[0])\n",
    "    send.sendto(struct.pack(\"i\", ret), (UDP_IP, UDP_PORT_SEND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class MountainCarTrain:\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.gamma=0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.05\n",
    "        self.epsilon_min=0.01\n",
    "        self.learingRate=0.001\n",
    "        self.replayBuffer=deque(maxlen=20000)\n",
    "        self.trainNetwork=self.createNetwork()\n",
    "        self.episodeNum=400\n",
    "        self.iterationNum=201 #max is 200\n",
    "        self.numPickFromBuffer=32\n",
    "        self.targetNetwork=self.createNetwork()\n",
    "        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n",
    "\n",
    "    def createNetwork(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_shape=(6700, )))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "    def getBestAction(self,state):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, 3)\n",
    "        else:\n",
    "            action=np.argmax(self.trainNetwork.predict(state)[0])\n",
    "        return action\n",
    "\n",
    "    def trainFromBuffer_Boost(self):\n",
    "        if len(self.replayBuffer) < self.numPickFromBuffer:\n",
    "            return\n",
    "        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n",
    "        npsamples = np.array(samples)\n",
    "        states_temp, actions_temp, rewards_temp, newstates_temp, dones_temp = np.hsplit(npsamples, 5)\n",
    "        states = np.concatenate((np.squeeze(states_temp[:])), axis = 0)\n",
    "        rewards = rewards_temp.reshape(self.numPickFromBuffer,).astype(float)\n",
    "        targets = self.trainNetwork.predict(states)\n",
    "        newstates = np.concatenate(np.concatenate(newstates_temp))\n",
    "        dones = np.concatenate(dones_temp).astype(bool)\n",
    "        notdones = ~dones\n",
    "        notdones = notdones.astype(float)\n",
    "        dones = dones.astype(float)\n",
    "        Q_futures = self.targetNetwork.predict(newstates).max(axis = 1)\n",
    "        targets[(np.arange(self.numPickFromBuffer), actions_temp.reshape(self.numPickFromBuffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n",
    "        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def trainFromBuffer(self):\n",
    "        if len(self.replayBuffer) < self.numPickFromBuffer:\n",
    "            return\n",
    "        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n",
    "        states = []\n",
    "        newStates=[]\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            states.append(state)\n",
    "            newStates.append(new_state)\n",
    "\n",
    "        newArray = np.array(states)\n",
    "        states = newArray.reshape(self.numPickFromBuffer, 2)\n",
    "        newArray2 = np.array(newStates)\n",
    "        newStates = newArray2.reshape(self.numPickFromBuffer, 2)\n",
    "        targets = self.trainNetwork.predict(states)\n",
    "        new_state_targets=self.targetNetwork.predict(newStates)\n",
    "\n",
    "        i=0\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = targets[i]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                Q_future = max(new_state_targets[i])\n",
    "                target[action] = reward + Q_future * self.gamma\n",
    "            i+=1\n",
    "\n",
    "        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def orginalTry(self,currentState,eps):\n",
    "        rewardSum = 0\n",
    "        max_position=-99\n",
    "\n",
    "        for i in range(self.iterationNum):\n",
    "            bestAction = self.getBestAction(currentState)\n",
    "\n",
    "            new_state, reward, done, _ = env.step(bestAction)\n",
    "            new_state = new_state.reshape(1, 2)\n",
    "\n",
    "            # # Keep track of max position\n",
    "            if new_state[0][0] > max_position:\n",
    "                max_position = new_state[0][0]\n",
    "\n",
    "            # # Adjust reward for task completion\n",
    "            if new_state[0][0] >= 0.5:\n",
    "                reward += 10\n",
    "\n",
    "            self.replayBuffer.append([currentState, bestAction, reward, new_state, done])\n",
    "\n",
    "            #Or you can use self.trainFromBuffer_Boost(), it is a matrix wise version for boosting \n",
    "            self.trainFromBuffer()\n",
    "\n",
    "            rewardSum += reward\n",
    "\n",
    "            currentState = new_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if i >= 199:\n",
    "            print(\"Failed to finish task in epsoide {}\".format(eps))\n",
    "        else:\n",
    "            print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n",
    "            self.trainNetwork.save('./trainNetworkInEPS{}.h5'.format(eps))\n",
    "\n",
    "        #Sync\n",
    "        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n",
    "\n",
    "        print(\"now epsilon is {}, the reward is {} maxPosition is {}\".format(max(self.epsilon_min, self.epsilon), rewardSum,max_position))\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "\n",
    "    def start(self):\n",
    "        for eps in range(self.episodeNum):\n",
    "            currentState=env.reset().reshape(1,2)\n",
    "            self.orginalTry(currentState, eps)\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "dqn=MountainCarTrain(env=env)\n",
    "dqn.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
