{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dealer/miniconda3/envs/ai2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Input, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "import socket\n",
    "import struct\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "data = np.array(df)\n",
    "\n",
    "x = data[:, :6700]\n",
    "y = data[:, 6700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(735, 6700)\n",
    "y = np.array([lst[int(i)] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(6700, )))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "epochs = 25\n",
    "sgd = SGD(lr=0.05, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x, y, batch_size=32, epochs=epochs)\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.argmax(model.predict_on_batch(np.random.rand(1, 6700)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train from realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.UDP_IP = \"127.0.0.1\"\n",
    "        self.UDP_PORT_RECV = 9003\n",
    "        self.UDP_PORT_SEND = 9004\n",
    "\n",
    "        self.send = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.recv = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.recv.bind((self.UDP_IP, self.UDP_PORT_RECV))\n",
    "\n",
    "    def reset(self):\n",
    "        self.send.sendto(struct.pack(\"i\", -1), (self.UDP_IP, self.UDP_PORT_SEND))\n",
    "        data, _ = self.recv.recvfrom(26808)\n",
    "        data = np.array(struct.unpack('6702f', data))[:6700]\n",
    "        return data.reshape(1, 6700)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.send.sendto(struct.pack(\"i\", action), (self.UDP_IP, self.UDP_PORT_SEND))\n",
    "        data, _ = self.recv.recvfrom(26808)\n",
    "        data = np.array(struct.unpack('6702f', data))\n",
    "        new_state = data[:6700].reshape(1, 6700)\n",
    "        reward = data[6700]\n",
    "        lose = True if data[6701] > 0 else False\n",
    "        return new_state, reward, lose\n",
    "\n",
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 0.01\n",
    "        self.epsilon_decay = 0.05\n",
    "        self.epsilon_min = 0.01\n",
    "        self.sgd = SGD(lr=0.02)\n",
    "        self.replay_buffer = deque(maxlen=20000)\n",
    "        self.train_network = self.create_model()\n",
    "        self.episode_num = 400\n",
    "        self.num_pick_from_buffer = 32\n",
    "        self.target_network = self.create_model()\n",
    "        self.target_network.set_weights(self.train_network.get_weights())\n",
    "        self.old_reward = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = load_model('Models/1_episodes_100.h5')\n",
    "#         model.add(Dense(32, activation='relu', input_shape=(6700, )))\n",
    "#         model.add(Dense(32, activation='relu'))\n",
    "#         model.add(Dense(3, activation='softmax'))\n",
    "#         last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "#         inputs = Input(shape=(6700,))\n",
    "#         out = Dense(128, activation=\"relu\")(inputs)    \n",
    "#         out = BatchNormalization()(out)\n",
    "#         out = Dense(32, activation=\"relu\")(out)\n",
    "#         outputs = Dense(units=3, activation='softmax', name='raw_actions', kernel_initializer=last_init)(out)\n",
    "#         model = Model(inputs=inputs, outputs=outputs)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=self.sgd, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, 3)\n",
    "        else:\n",
    "            action = np.argmax(self.train_network.predict(state)[0])\n",
    "        return action\n",
    "\n",
    "    def train_from_buffer(self):\n",
    "        if len(self.replay_buffer) < self.num_pick_from_buffer:\n",
    "            return\n",
    "        samples = random.sample(self.replay_buffer, self.num_pick_from_buffer)\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            states.append(state)\n",
    "            new_states.append(new_state)\n",
    "\n",
    "        new_array = np.array(states)\n",
    "        states = new_array.reshape(self.num_pick_from_buffer, 6700)\n",
    "        new_array2 = np.array(new_states)\n",
    "        new_states = new_array2.reshape(self.num_pick_from_buffer, 6700)\n",
    "        targets = self.train_network.predict(states)\n",
    "        new_state_targets = self.target_network.predict(new_states)\n",
    "\n",
    "        i = 0\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = targets[i]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                Q_future = max(new_state_targets[i])\n",
    "                target[action] = reward + Q_future * self.gamma\n",
    "            i += 1\n",
    "\n",
    "        self.train_network.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def original_try(self, current_state, eps):\n",
    "        reward_sum = 0\n",
    "        iteration = 0\n",
    "\n",
    "        while True:\n",
    "            best_action = self.get_best_action(current_state)\n",
    "            new_state, reward, lose = self.env.step(best_action)\n",
    "            done = False\n",
    "            if reward > 0.8:\n",
    "                done = True\n",
    "            if iteration > 30 and iteration % 2 == 0:\n",
    "                self.replay_buffer.append([current_state, best_action, reward, new_state, done])\n",
    "            self.train_from_buffer()\n",
    "            reward_sum += reward\n",
    "            current_state = new_state\n",
    "            \n",
    "            iteration +=1\n",
    "            \n",
    "            if lose:\n",
    "                break\n",
    "\n",
    "        self.target_network.set_weights(self.train_network.get_weights())\n",
    "\n",
    "        print(f\"ep: {eps}, epsilon: {self.epsilon:.02f}, iteration: {iteration}, reward: {reward_sum}\")\n",
    "        if self.old_reward > reward_sum - 0.1:\n",
    "            self.epsilon += random.uniform(0.03, 0.08)\n",
    "        else:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.old_reward = reward_sum\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "    def start(self):\n",
    "        for eps in range(self.episode_num):\n",
    "            current_state = self.env.reset()\n",
    "            self.original_try(current_state, eps)\n",
    "\n",
    "\n",
    "train = Train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, epsilon: 0.01, iteration: 502, reward: 508.5999842360616\n",
      "ep: 1, epsilon: 0.01, iteration: 340, reward: 93.00000566244125\n",
      "ep: 2, epsilon: 0.10, iteration: 345, reward: 233.72000761330128\n",
      "ep: 3, epsilon: 0.05, iteration: 502, reward: 501.8000223338604\n",
      "ep: 4, epsilon: 0.01, iteration: 509, reward: 410.35999834537506\n",
      "ep: 5, epsilon: 0.04, iteration: 564, reward: 488.5999894142151\n",
      "ep: 6, epsilon: 0.01, iteration: 509, reward: 389.8799983263016\n",
      "ep: 7, epsilon: 0.08, iteration: 502, reward: 417.2400000989437\n",
      "ep: 8, epsilon: 0.03, iteration: 509, reward: 397.55999833345413\n",
      "ep: 9, epsilon: 0.06, iteration: 723, reward: 774.5999469161034\n",
      "ep: 10, epsilon: 0.01, iteration: 340, reward: 50.19999647140503\n",
      "ep: 11, epsilon: 0.06, iteration: 502, reward: 459.16001556813717\n",
      "ep: 12, epsilon: 0.01, iteration: 340, reward: 50.19999647140503\n",
      "ep: 13, epsilon: 0.10, iteration: 502, reward: 491.79998575150967\n",
      "ep: 14, epsilon: 0.05, iteration: 502, reward: 474.1200164631009\n",
      "ep: 15, epsilon: 0.15, iteration: 502, reward: 423.6400004029274\n",
      "ep: 16, epsilon: 0.19, iteration: 502, reward: 472.76001638174057\n",
      "ep: 17, epsilon: 0.14, iteration: 502, reward: 415.960000038147\n",
      "ep: 18, epsilon: 0.21, iteration: 502, reward: 519.4799844101071\n",
      "ep: 19, epsilon: 0.16, iteration: 502, reward: 500.91998489946127\n",
      "ep: 20, epsilon: 0.20, iteration: 502, reward: 461.88001573085785\n",
      "ep: 21, epsilon: 0.26, iteration: 340, reward: 94.20000576972961\n",
      "ep: 22, epsilon: 0.30, iteration: 502, reward: 469.64001855254173\n",
      "ep: 23, epsilon: 0.25, iteration: 502, reward: 486.5200206339359\n",
      "ep: 24, epsilon: 0.20, iteration: 502, reward: 463.96001609414816\n",
      "ep: 25, epsilon: 0.27, iteration: 502, reward: 421.9600003659725\n",
      "ep: 26, epsilon: 0.32, iteration: 502, reward: 463.2400158122182\n",
      "ep: 27, epsilon: 0.27, iteration: 502, reward: 412.11999985575676\n",
      "ep: 28, epsilon: 0.35, iteration: 502, reward: 417.2400000989437\n",
      "ep: 29, epsilon: 0.30, iteration: 503, reward: 418.9999999701977\n",
      "ep: 30, epsilon: 0.25, iteration: 502, reward: 452.36001516133547\n",
      "ep: 31, epsilon: 0.20, iteration: 502, reward: 456.4400154054165\n",
      "ep: 32, epsilon: 0.15, iteration: 502, reward: 471.40001630038023\n",
      "ep: 33, epsilon: 0.10, iteration: 509, reward: 361.7199977636337\n",
      "ep: 34, epsilon: 0.19, iteration: 318, reward: 114.44000571966171\n",
      "ep: 35, epsilon: 0.23, iteration: 502, reward: 453.7200152426958\n",
      "ep: 36, epsilon: 0.18, iteration: 564, reward: 496.27998942136765\n",
      "ep: 37, epsilon: 0.13, iteration: 502, reward: 463.2400158122182\n",
      "ep: 38, epsilon: 0.21, iteration: 502, reward: 470.0400162190199\n",
      "ep: 39, epsilon: 0.16, iteration: 502, reward: 471.40001630038023\n",
      "ep: 40, epsilon: 0.11, iteration: 509, reward: 411.63999834656715\n",
      "ep: 41, epsilon: 0.19, iteration: 502, reward: 480.9199855774641\n",
      "ep: 42, epsilon: 0.14, iteration: 502, reward: 461.88001573085785\n",
      "ep: 43, epsilon: 0.20, iteration: 502, reward: 457.80001548677683\n",
      "ep: 44, epsilon: 0.28, iteration: 502, reward: 464.60001589357853\n",
      "ep: 45, epsilon: 0.23, iteration: 564, reward: 497.55998942255974\n",
      "ep: 46, epsilon: 0.18, iteration: 502, reward: 455.08001532405615\n",
      "ep: 47, epsilon: 0.22, iteration: 502, reward: 475.48001654446125\n",
      "ep: 48, epsilon: 0.17, iteration: 502, reward: 419.8000002205372\n",
      "ep: 49, epsilon: 0.27, iteration: 502, reward: 461.480016566813\n",
      "ep: 50, epsilon: 0.22, iteration: 502, reward: 459.16001556813717\n",
      "ep: 51, epsilon: 0.28, iteration: 509, reward: 387.3199983239174\n",
      "ep: 52, epsilon: 0.36, iteration: 502, reward: 456.4400154054165\n",
      "ep: 53, epsilon: 0.31, iteration: 502, reward: 486.2000198289752\n",
      "ep: 54, epsilon: 0.26, iteration: 502, reward: 412.11999985575676\n",
      "ep: 55, epsilon: 0.30, iteration: 502, reward: 409.5599997341633\n",
      "ep: 56, epsilon: 0.34, iteration: 502, reward: 468.68001613765955\n",
      "ep: 57, epsilon: 0.29, iteration: 509, reward: 398.8399983346462\n",
      "ep: 58, epsilon: 0.32, iteration: 564, reward: 471.959989130497\n",
      "ep: 59, epsilon: 0.27, iteration: 502, reward: 414.67999997735023\n",
      "ep: 60, epsilon: 0.34, iteration: 502, reward: 455.08001532405615\n",
      "ep: 61, epsilon: 0.29, iteration: 502, reward: 489.0799857079983\n",
      "ep: 62, epsilon: 0.24, iteration: 502, reward: 453.7200152426958\n",
      "ep: 63, epsilon: 0.29, iteration: 502, reward: 473.5600190013647\n",
      "ep: 64, epsilon: 0.24, iteration: 502, reward: 497.23998583853245\n",
      "ep: 65, epsilon: 0.19, iteration: 502, reward: 463.2400158122182\n",
      "ep: 66, epsilon: 0.28, iteration: 502, reward: 455.08001532405615\n",
      "ep: 67, epsilon: 0.34, iteration: 504, reward: 422.0399999022484\n",
      "ep: 68, epsilon: 0.38, iteration: 502, reward: 468.36001897603273\n",
      "ep: 69, epsilon: 0.33, iteration: 502, reward: 449.5600153282285\n",
      "ep: 70, epsilon: 0.39, iteration: 502, reward: 495.4799858406186\n",
      "ep: 71, epsilon: 0.34, iteration: 502, reward: 471.40001630038023\n",
      "ep: 72, epsilon: 0.41, iteration: 345, reward: 228.92000749707222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5aa6d0703151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-99e0fd98907e>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_try\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-99e0fd98907e>\u001b[0m in \u001b[0;36moriginal_try\u001b[0;34m(self, current_state, eps)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fa9311b9e9b5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUDP_IP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUDP_PORT_SEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m26808\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6702f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6700\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_network.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.send.close()\n",
    "env.recv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Models/1_episodes_100.h5\")\n",
    "\n",
    "UDP_IP = \"127.0.0.1\"\n",
    "UDP_PORT_RECV = 9003\n",
    "UDP_PORT_SEND = 9004\n",
    "\n",
    "\n",
    "send = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "recv = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "recv.bind((UDP_IP, UDP_PORT_RECV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send.sendto(struct.pack(\"i\", -1), (UDP_IP, UDP_PORT_SEND))\n",
    "\n",
    "while True:\n",
    "    data, addr = recv.recvfrom(26808)\n",
    "    inp = struct.unpack('6702f', data)[:6700]\n",
    "    inp = np.array(inp).reshape(1, 6700)\n",
    "    ret = model.predict(inp)\n",
    "    ret = np.argmax(ret[0])\n",
    "    send.sendto(struct.pack(\"i\", ret), (UDP_IP, UDP_PORT_SEND))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
